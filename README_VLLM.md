# vLLM Distributed Cluster con vibe.py

Sistema distribuido para ejecutar modelos LLM usando vLLM en modo master/worker con integraci√≥n a vibe.py.

## üìã Requisitos

- **Windows 11** con WSL2 Ubuntu
- **GPU NVIDIA** con soporte CUDA
- **Python 3.9-3.12** en WSL
- **CUDA Toolkit** instalado en WSL

## üöÄ Instalaci√≥n R√°pida

### 1. Instalar vLLM en WSL Ubuntu

```bash
# Desde Windows, abre WSL Ubuntu
wsl -d Ubuntu

# Navega al directorio del proyecto
cd /mnt/c/Users/herna/OneDrive/proyects/converge

# Da permisos y ejecuta el instalador
chmod +x install_vllm.sh
./install_vllm.sh
```

El script instalar√°:
- Python 3 y dependencias
- CUDA Toolkit (nvcc)
- vLLM y Ray
- Entorno virtual en `~/vllm_workspace`

### 2. Verificar Instalaci√≥n

```bash
# Dentro de WSL
cd ~/vllm_workspace
source vllm_env/bin/activate
python -c "import vllm, ray; print('vLLM:', vllm.__version__)"
```

## üéØ Uso

### Modo 1: Nodo Master (Servidor Principal)

El nodo master ejecuta el modelo y coordina workers.

```bash
# Desde Windows
wsl -d Ubuntu

# Navega al proyecto
cd /mnt/c/Users/herna/OneDrive/proyects/converge

# Inicia el master
./start_vllm_master.sh [MODELO] [PUERTO] [NUM_GPUS]
```

**Ejemplos:**

```bash
# Modelo peque√±o para pruebas (125M par√°metros)
./start_vllm_master.sh facebook/opt-125m 8000 1

# Llama 2 7B (requiere ~14GB VRAM)
./start_vllm_master.sh meta-llama/Llama-2-7b-hf 8000 1

# Mistral 7B
./start_vllm_master.sh mistralai/Mistral-7B-v0.1 8000 1
```

**El master mostrar√°:**
- IP del master (ej: `172.22.23.97`)
- Puerto Ray: `6379`
- Puerto API: `8000`
- Dashboard Ray: `http://localhost:8265`

### Modo 2: Nodo Worker (Esclavo)

Los workers se conectan al master para distribuir la carga.

```bash
# En otra m√°quina o terminal WSL
./start_vllm_worker.sh <IP_MASTER> [PUERTO_RAY]
```

**Ejemplo:**

```bash
./start_vllm_worker.sh 172.22.23.97 6379
```

### Verificar el Cluster

**Ray Dashboard:**
- Abre en navegador: `http://localhost:8265` (si est√°s en el master)
- Ver√°s todos los nodos conectados

**Verificar API:**

```bash
# Probar que el servidor responde
curl http://localhost:8000/v1/models

# Hacer una inferencia
curl http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "facebook/opt-125m",
    "prompt": "Hello, my name is",
    "max_tokens": 50,
    "temperature": 0.7
  }'
```

## üîó Integraci√≥n con vibe.py

Una vez el master est√° corriendo, usa `vibe.py` para procesar requests:

```python
# vibe.py se conecta al servidor vLLM
python vibe.py
```

Configura vibe.py para apuntar al endpoint vLLM:
```python
# En vibe.py, configurar:
base_url = "http://localhost:8000/v1"
model_name = "facebook/opt-125m"  # o el modelo que uses
```

## üõ†Ô∏è Comandos √ötiles

### Detener el Cluster

```bash
# Detiene Ray y vLLM
./stop_vllm.sh
```

### Ver Estado del Cluster

```bash
# Activar entorno
cd ~/vllm_workspace
source vllm_env/bin/activate

# Ver estado de Ray
ray status

# Ver procesos vLLM
ps aux | grep vllm
```

### Verificar GPU

```bash
# Ver uso de GPU
nvidia-smi

# Monitoreo continuo
watch -n 1 nvidia-smi
```

### Logs y Debugging

```bash
# Ver logs de Ray
cat /tmp/ray/session_latest/logs/*

# Variables de entorno para debug
export VLLM_LOGGING_LEVEL=DEBUG
export RAY_VERBOSE=1
```

## üìä Modelos Recomendados

### Para Pruebas (sin GPU potente):
- `facebook/opt-125m` (125M par√°metros, ~500MB)
- `facebook/opt-350m` (350M par√°metros, ~1.3GB)
- `gpt2` (124M par√°metros, ~500MB)

### Para Producci√≥n (requiere GPU):
- `meta-llama/Llama-2-7b-hf` (7B par√°metros, ~14GB VRAM)
- `meta-llama/Llama-2-13b-hf` (13B par√°metros, ~26GB VRAM)
- `mistralai/Mistral-7B-v0.1` (7B par√°metros, ~14GB VRAM)
- `mistralai/Mixtral-8x7B-v0.1` (47B par√°metros, ~90GB VRAM)

## üîß Soluci√≥n de Problemas

### Error: "Permission denied: 'nvcc'"

```bash
# Instalar CUDA Toolkit
wsl -d Ubuntu
sudo apt-get update
sudo apt-get install -y nvidia-cuda-toolkit
nvcc --version
```

### Error: "CUDA out of memory"

- Usa un modelo m√°s peque√±o
- Reduce `--max-model-len`:
  ```bash
  ./start_vllm_master.sh facebook/opt-125m 8000 1 --max-model-len 1024
  ```

### Error: "Cannot connect to Ray"

- Verifica que el master est√© corriendo: `ray status`
- Revisa la IP del master: `hostname -I`
- Asegura que el firewall permita puerto 6379

### Workers no aparecen

- Verifica que ambas m√°quinas est√©n en la misma red
- Revisa firewall (puertos 6379, 8265)
- Usa la IP correcta del master (no `localhost`)

### Modelo no se descarga

- Requiere conexi√≥n a internet
- Los modelos se guardan en `~/.cache/huggingface/`
- Para modelos privados (Llama 2):
  ```bash
  huggingface-cli login
  ```

## üìÅ Estructura de Archivos

```
converge/
‚îú‚îÄ‚îÄ install_vllm.sh           # Instalador de vLLM en WSL
‚îú‚îÄ‚îÄ start_vllm_master.sh      # Iniciar nodo master
‚îú‚îÄ‚îÄ start_vllm_worker.sh      # Iniciar nodo worker
‚îú‚îÄ‚îÄ stop_vllm.sh              # Detener cluster
‚îú‚îÄ‚îÄ vibe.py                   # Cliente para procesar requests
‚îî‚îÄ‚îÄ README_VLLM.md           # Esta gu√≠a
```

## üåê API Compatible con OpenAI

vLLM expone una API compatible con OpenAI:

```python
from openai import OpenAI

client = OpenAI(
    api_key="EMPTY",
    base_url="http://localhost:8000/v1"
)

response = client.completions.create(
    model="facebook/opt-125m",
    prompt="Once upon a time",
    max_tokens=50
)

print(response.choices[0].text)
```

## üìù Notas

- El primer inicio descarga el modelo (puede tardar varios minutos)
- Los modelos se cachean en `~/.cache/huggingface/`
- WSL2 comparte la GPU de Windows autom√°ticamente
- Para m√∫ltiples GPUs, aumenta `--tensor-parallel-size`

## üîó Enlaces √ötiles

- [vLLM Docs](https://docs.vllm.ai/)
- [Ray Docs](https://docs.ray.io/)
- [Hugging Face Models](https://huggingface.co/models)
